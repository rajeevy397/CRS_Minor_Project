# -*- coding: utf-8 -*-
"""building logistic regression model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-2Ew7Mg8fSnIcMW_HQH2EoKFLe5QGANp

Steps involved in building  logistic regession in python

1.set learning rate and no. of iterations; initiate the the random weight and random value.

2.Build the logistic Regression function(Sigmoid function)

3.Update the parameters Using the gradient descent

Finally we will get the best model (best weight and bias) as it has minimum cost function

4.Build the predict function to determine the class of data point.
"""

#importing libraries
import numpy as np

class Logistic_Regression() :
  
  #declaring learning rate and no.of parameters(hyperparameters)
  def __init__(self,learning_rate,no_of_iterations):

    self.learning_rate = learning_rate
    self.no_of_iterations = no_of_iterations

  #fit the function to train the model with datasets
  def fit(self, X, Y):

    #no. of data points in the dataset (no. of rows) ------ -> m
    #no. of input features in the datasets (no of column) ---> n
    self.m, self.n = X.shape

    #initiating the weight and bias
    self.w = np.zeros(self.n)   # "np.zero" means array containing all values (i.e. n) equal to 0.
    self.b = 0
    self.X = X
    self.Y = Y

    #implementing gradient discent for optimization
    for i in range(self.no_of_iterations):
      self.update_weights()

  def update_weights(self):

    #y_hat formulae(sigmoid function)
    Y_hat = 1/(1 + np.exp(-(self.X.dot(self.w) + self.b)))

    #derivatives
    dw = (1/self.m)*np.dot(self.X.T, (Y_hat-self.Y))
    db = (1/self.m)*np.sum(Y_hat-self.Y)

    #updating the weights and bias
    self.w = self.w - self.learning_rate*dw
    self.b = self.b - self.learning_rate*db

  def predict(self,X):
    Y_pred = 1/(1 + np.exp(-(X.dot(self.w) + self.b)))
    Y_pred = np.where(Y_pred > 0.5 , 1, 0)
    return Y_pred